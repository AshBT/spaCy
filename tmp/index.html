

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>spaCy: Industrial-strength NLP &mdash; spaCy 0.85 documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Roboto+Slab:400,700|Inconsolata:400,700&subset=latin,cyrillic' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="spaCy 0.85 documentation" href="#"/>
        <link rel="next" title="Quick Start" href="quickstart.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-58931649-1', 'gini.net');
      ga('send', 'pageview');
  </script>
  

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        
          <a href="#" class="fa fa-home"> spaCy</a>
        
        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#install">Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#usage">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#most-of-the-api-at-a-glance">(Most of the) API at a glance</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#features">Features</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reference/index.html">Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="reference/processing.html">spacy.en.English</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/using/document.html">The Doc Object</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/using/span.html">The Span Object</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/using/token.html">The Token Object</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/using/lexeme.html">The Lexeme Object</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/lookup.html">Lexical Lookup</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a><ul>
<li class="toctree-l2"><a class="reference internal" href="license.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="updates.html">Updates</a><ul>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-89">v0.89</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-88">2015-07-08 v0.88</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-87">2015-07-01 v0.87</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-86">2015-06-24 v0.86</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-85">2015-05-12 v0.85</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-84">2015-05-12 v0.84</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-80">2015-04-13 v0.80</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-70">2015-03-05 v0.70</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#spacy-v0-4-still-alpha-improving-quickly">2015-01-30 spaCy v0.4: Still alpha, improving quickly</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#bug-fixes">Bug Fixes</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#known-issues">Known Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#enhancements-train-and-evaluate-on-whole-paragraphs">Enhancements: Train and evaluate on whole paragraphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#id1">2015-01-25</a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="#">spaCy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="#">Docs</a> &raquo;</li>
      
    <li>spaCy: Industrial-strength NLP</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/index.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <div class="section" id="spacy-industrial-strength-nlp">
<h1>spaCy: Industrial-strength NLP<a class="headerlink" href="#spacy-industrial-strength-nlp" title="Permalink to this headline">¶</a></h1>
<p><strong>2015-07-08</strong>: <a href="#id1"><span class="problematic" id="id2">`Version 0.88 released`_</span></a></p>
<p><a class="reference external" href="https://github.com/honnibal/spaCy/">spaCy</a> is a new library for text processing in Python and Cython.
I wrote it because I think small companies are terrible at
natural language processing (NLP).  Or rather:
small companies are using terrible NLP technology.</p>
<p>To do great NLP, you have to know a little about linguistics, a lot
about machine learning, and almost everything about the latest research.
The people who fit this description seldom join small companies.
Most are broke &#8212; they&#8217;ve just finished grad school.
If they don&#8217;t want to stay in academia, they join Google, IBM, etc.</p>
<p>The net result is that outside of the tech giants, commercial NLP has changed
little in the last ten years.  In academia, it&#8217;s changed entirely.  Amazing
improvements in quality.  Orders of magnitude faster.  But the
academic code is always GPL, undocumented, unuseable, or all three.  You could
implement the ideas yourself, but the papers are hard to read, and training
data is exorbitantly expensive.  So what are you left with?  A common answer is
NLTK, which was written primarily as an educational resource.  Nothing past the
tokenizer is suitable for production use.</p>
<p>I used to think that the NLP community just needed to do more to communicate
its findings to software engineers.  So I wrote two blog posts, explaining
<a class="reference external" href="https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/">how to write a part-of-speech tagger</a> and <a class="reference external" href="https://honnibal.wordpress.com/2013/12/18/a-simple-fast-algorithm-for-natural-language-dependency-parsing/">parser</a>.  Both were well received,
and there&#8217;s been a bit of interest in <a class="reference external" href="https://github.com/syllog1sm/redshift/tree/develop">my research software</a> &#8212; even though
it&#8217;s entirely undocumented, and mostly unuseable to anyone but me.</p>
<p>So six months ago I quit my post-doc, and I&#8217;ve been working day and night on
spaCy since.  I&#8217;m now pleased to announce an alpha release.</p>
<p>If you&#8217;re a small company doing NLP, I think spaCy will seem like a minor miracle.
It&#8217;s by far the fastest NLP software ever released.
The full processing pipeline completes in 20ms per document, including accurate
tagging and parsing.  All strings are mapped to integer IDs, tokens are linked
to embedded word representations, and a range of useful features are pre-calculated
and cached.</p>
<p>If none of that made any sense to you, here&#8217;s the gist of it.  Computers don&#8217;t
understand text.  This is unfortunate, because that&#8217;s what the web almost entirely
consists of.  We want to recommend people text based on other text they liked.
We want to shorten text to display it on a mobile screen.  We want to aggregate
it, link it, filter it, categorise it, generate it and correct it.</p>
<p>spaCy provides a library of utility functions that help programmers build such
products.  It&#8217;s commercial open source software: you can either use it under
the AGPL, or you can <a class="reference external" href="license.html">buy a commercial license</a> for a one-time fee.</p>
<div class="section" id="example-functionality">
<h2>Example functionality<a class="headerlink" href="#example-functionality" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s say you&#8217;re developing a proofreading tool, or possibly an IDE for
writers.  You&#8217;re convinced by Stephen King&#8217;s advice that <a class="reference external" href="http://www.brainpickings.org/2013/03/13/stephen-king-on-adverbs/">adverbs are not your
friend</a>, so
you want to <strong>highlight all adverbs</strong>.  We&#8217;ll use one of the examples he finds
particularly egregious:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">spacy.en</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spacy.parts_of_speech</span> <span class="kn">import</span> <span class="n">ADV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># Load the pipeline, and call it with some text.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">English</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokens</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s">u&quot;‘Give it back,’ he pleaded abjectly, ‘it’s mine.’&quot;</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">parse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">u&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tok</span><span class="o">.</span><span class="n">string</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">if</span> <span class="n">tok</span><span class="o">.</span><span class="n">pos</span> <span class="o">==</span> <span class="n">ADV</span> <span class="k">else</span> <span class="n">tok</span><span class="o">.</span><span class="n">string</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">)</span>
<span class="go">u‘Give it BACK,’ he pleaded ABJECTLY, ‘it’s mine.’</span>
</pre></div>
</div>
<p>Easy enough &#8212; but the problem is that we&#8217;ve also highlighted &#8220;back&#8221;.
While &#8220;back&#8221; is undoubtedly an adverb, we probably don&#8217;t want to highlight it.
If what we&#8217;re trying to do is flag dubious stylistic choices, we&#8217;ll need to
refine our logic.  It turns out only a certain type of adverb is of interest to
us.</p>
<p>There are lots of ways we might do this, depending on just what words
we want to flag.  The simplest way to exclude adverbs like &#8220;back&#8221; and &#8220;not&#8221;
is by word frequency: these words are much more common than the prototypical
manner adverbs that the style guides are worried about.</p>
<p>The <a class="reference internal" href="reference/using/lexeme.html#Lexeme.prob" title="Lexeme.prob"><tt class="xref py py-attr docutils literal"><span class="pre">Lexeme.prob</span></tt></a> and <a class="reference internal" href="reference/using/token.html#Token.prob" title="Token.prob"><tt class="xref py py-attr docutils literal"><span class="pre">Token.prob</span></tt></a> attribute gives a
log probability estimate of the word:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s">u&#39;back&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prob</span>
<span class="go">-7.403977394104004</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s">u&#39;not&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prob</span>
<span class="go">-5.407193660736084</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s">u&#39;quietly&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prob</span>
<span class="go">-11.07155704498291</span>
</pre></div>
</div>
<p>(The probability estimate is based on counts from a 3 billion word corpus,
smoothed using the <a class="reference external" href="http://www.d.umn.edu/~tpederse/Courses/CS8761-FALL02/Code/sgt-gale.pdf">Simple Good-Turing</a> method.)</p>
<p>So we can easily exclude the N most frequent words in English from our adverb
marker.  Let&#8217;s try N=1000 for now:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">spacy.en</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spacy.parts_of_speech</span> <span class="kn">import</span> <span class="n">ADV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">English</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># Find log probability of Nth most frequent word</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">lex</span><span class="o">.</span><span class="n">prob</span> <span class="k">for</span> <span class="n">lex</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probs</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_adverb</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">tok</span><span class="p">:</span> <span class="n">tok</span><span class="o">.</span><span class="n">pos</span> <span class="o">==</span> <span class="n">ADV</span> <span class="ow">and</span> <span class="n">tok</span><span class="o">.</span><span class="n">prob</span> <span class="o">&lt;</span> <span class="n">probs</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokens</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s">u&quot;‘Give it back,’ he pleaded abjectly, ‘it’s mine.’&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">u&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tok</span><span class="o">.</span><span class="n">string</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">if</span> <span class="n">is_adverb</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="k">else</span> <span class="n">tok</span><span class="o">.</span><span class="n">string</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">)</span>
<span class="go">‘Give it back,’ he pleaded ABJECTLY, ‘it’s mine.’</span>
</pre></div>
</div>
<p>There are lots of other ways we could refine the logic, depending on just what
words we want to flag.  Let&#8217;s say we wanted to only flag adverbs that modified words
similar to &#8220;pleaded&#8221;.  This is easy to do, as spaCy loads a vector-space
representation for every word (by default, the vectors produced by
<a class="reference external" href="https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/">Levy and Goldberg (2014)</a>).  Naturally, the vector is provided as a numpy
array:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">pleaded</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pleaded</span><span class="o">.</span><span class="n">repvec</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(300,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pleaded</span><span class="o">.</span><span class="n">repvec</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
<span class="go">array([ 0.04229792,  0.07459262,  0.00820188, -0.02181299,  0.07519238], dtype=float32)</span>
</pre></div>
</div>
<p>We want to sort the words in our vocabulary by their similarity to &#8220;pleaded&#8221;.
There are lots of ways to measure the similarity of two vectors.  We&#8217;ll use the
cosine metric:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cosine</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">:</span> <span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">v2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span> <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">has_repvec</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">words</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">cosine</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">repvec</span><span class="p">,</span> <span class="n">pleaded</span><span class="o">.</span><span class="n">repvec</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">words</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;1-20&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]))</span>
<span class="go">1-20 pleaded, pled, plead, confessed, interceded, pleads, testified, conspired, motioned, demurred, countersued, remonstrated, begged, apologised, consented, acquiesced, petitioned, quarreled, appealed, pleading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;50-60&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">60</span><span class="p">]))</span>
<span class="go">50-60 counselled, bragged, backtracked, caucused, refiled, dueled, mused, dissented, yearned, confesses</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;100-110&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">110</span><span class="p">]))</span>
<span class="go">100-110 cabled, ducked, sentenced, perjured, absconded, bargained, overstayed, clerked, confided, sympathizes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;1000-1010&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">1000</span><span class="p">:</span><span class="mi">1010</span><span class="p">]))</span>
<span class="go">1000-1010 scorned, baled, righted, requested, swindled, posited, firebombed, slimed, deferred, sagged</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;50000-50010&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">50000</span><span class="p">:</span><span class="mi">50010</span><span class="p">]))</span>
<span class="go">50000-50010, fb, ford, systems, puck, anglers, ik, tabloid, dirty, rims, artists</span>
</pre></div>
</div>
<p>As you can see, the similarity model that these vectors give us is excellent
&#8212; we&#8217;re still getting meaningful results at 1000 words, off a single
prototype!  The only problem is that the list really contains two clusters of
words: one associated with the legal meaning of &#8220;pleaded&#8221;, and one for the more
general sense.  Sorting out these clusters is an area of active research.</p>
<p>A simple work-around is to average the vectors of several words, and use that
as our target:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">say_verbs</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;pleaded&#39;</span><span class="p">,</span> <span class="s">&#39;confessed&#39;</span><span class="p">,</span> <span class="s">&#39;remonstrated&#39;</span><span class="p">,</span> <span class="s">&#39;begged&#39;</span><span class="p">,</span> <span class="s">&#39;bragged&#39;</span><span class="p">,</span> <span class="s">&#39;confided&#39;</span><span class="p">,</span> <span class="s">&#39;requested&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">say_vector</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">nlp</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">verb</span><span class="p">]</span><span class="o">.</span><span class="n">repvec</span> <span class="k">for</span> <span class="n">verb</span> <span class="ow">in</span> <span class="n">say_verbs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">say_verbs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">words</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">cosine</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">repvec</span> <span class="o">*</span> <span class="n">say_vector</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">words</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;1-20&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]))</span>
<span class="go">1-20 bragged, remonstrated, enquired, demurred, sighed, mused, intimated, retorted, entreated, motioned, ranted, confided, countersued, gestured, implored, interceded, muttered, marvelled, bickered, despaired</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;50-60&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">60</span><span class="p">]))</span>
<span class="go">50-60 flaunted, quarrelled, ingratiated, vouched, agonized, apologised, lunched, joked, chafed, schemed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;1000-1010&#39;</span><span class="p">,</span> <span class="s">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">orth_</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="mi">1000</span><span class="p">:</span><span class="mi">1010</span><span class="p">]))</span>
<span class="go">1000-1010 hoarded, waded, ensnared, clamoring, abided, deploring, shriveled, endeared, rethought, berate</span>
</pre></div>
</div>
<p>These definitely look like words that King might scold a writer for attaching
adverbs to.  Recall that our original adverb highlighting function looked like
this:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">spacy.en</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spacy.parts_of_speech</span> <span class="kn">import</span> <span class="n">ADV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># Load the pipeline, and call it with some text.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">en</span><span class="o">.</span><span class="n">English</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokens</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s">&quot;‘Give it back,’ he pleaded abjectly, ‘it’s mine.’&quot;</span><span class="p">,</span>
<span class="go">                 tag=True, parse=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tok</span><span class="o">.</span><span class="n">string</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">if</span> <span class="n">tok</span><span class="o">.</span><span class="n">pos</span> <span class="o">==</span> <span class="n">ADV</span> <span class="k">else</span> <span class="n">tok</span><span class="o">.</span><span class="n">string</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">))</span>
<span class="go">‘Give it BACK,’ he pleaded ABJECTLY, ‘it’s mine.’</span>
</pre></div>
</div>
<p>We wanted to refine the logic so that only adverbs modifying evocative verbs
of communication, like &#8220;pleaded&#8221;, were highlighted.  We&#8217;ve now built a vector that
represents that type of word, so now we can highlight adverbs based on
subtle logic, honing in on adverbs that seem the most stylistically
problematic, given our starting assumptions:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">spacy.en</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">spacy.parts_of_speech</span> <span class="kn">import</span> <span class="n">ADV</span><span class="p">,</span> <span class="n">VERB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cosine</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">:</span> <span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">v2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">is_bad_adverb</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">target_verb</span><span class="p">,</span> <span class="n">tol</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">pos</span> <span class="o">!=</span> <span class="n">ADV</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">False</span>
<span class="gp">... </span>  <span class="k">elif</span> <span class="n">token</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">pos</span> <span class="o">!=</span> <span class="n">VERB</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">False</span>
<span class="gp">... </span>  <span class="k">elif</span> <span class="n">cosine</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">repvec</span><span class="p">,</span> <span class="n">target_verb</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">False</span>
<span class="gp">... </span>  <span class="k">else</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">True</span>
</pre></div>
</div>
<p>This example was somewhat contrived &#8212; and, truth be told, I&#8217;ve never really
bought the idea that adverbs were a grave stylistic sin.  But hopefully it got
the message across: the state-of-the-art NLP technologies are very powerful.
spaCy gives you easy and efficient access to them, which lets you build all
sorts of use products and features that were previously impossible.</p>
</div>
<div class="section" id="independent-evaluation">
<h2>Independent Evaluation<a class="headerlink" href="#independent-evaluation" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils">
<caption>Independent evaluation by Yahoo! Labs and Emory
University, to appear at ACL 2015. Higher is better.</caption>
<colgroup>
<col width="31%" />
<col width="23%" />
<col width="23%" />
<col width="23%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>System</td>
<td>Language</td>
<td>Accuracy</td>
<td>Speed</td>
</tr>
<tr class="row-even"><td>spaCy v0.86</td>
<td>Cython</td>
<td>91.9</td>
<td><strong>13,963</strong></td>
</tr>
<tr class="row-odd"><td>ClearNLP</td>
<td>Java</td>
<td>91.7</td>
<td>10,271</td>
</tr>
<tr class="row-even"><td>spaCy v0.84</td>
<td>Cython</td>
<td>90.9</td>
<td>13,963</td>
</tr>
<tr class="row-odd"><td>CoreNLP</td>
<td>Java</td>
<td>89.6</td>
<td>8,602</td>
</tr>
<tr class="row-even"><td>MATE</td>
<td>Java</td>
<td><strong>92.5</strong></td>
<td>550</td>
</tr>
<tr class="row-odd"><td>Turbo</td>
<td>C++</td>
<td>92.4</td>
<td>349</td>
</tr>
<tr class="row-even"><td>Yara</td>
<td>Java</td>
<td>92.3</td>
<td>340</td>
</tr>
</tbody>
</table>
<p>Accuracy is % unlabelled arcs correct, speed is tokens per second.</p>
<p>Joel Tetreault and Amanda Stent (Yahoo! Labs) and Jin-ho Choi (Emory) performed
a detailed comparison of the best parsers available.  All numbers above
are taken from the pre-print they kindly made available to me,
except for spaCy v0.86.</p>
<p>I&#8217;m particularly grateful to the authors for discussion of their results, which
led to the improvement in accuracy between v0.84 and v0.86.  A tip from Jin-ho
(developer of ClearNLP) was particularly useful.</p>
</div>
<div class="section" id="detailed-speed-comparison">
<h2>Detailed Speed Comparison<a class="headerlink" href="#detailed-speed-comparison" title="Permalink to this headline">¶</a></h2>
<p><strong>Set up</strong>: 100,000 plain-text documents were streamed from an SQLite3
database, and processed with an NLP library, to one of three levels of detail
&#8212; tokenization, tagging, or parsing.  The tasks are additive: to parse the
text you have to tokenize and tag it.  The  pre-processing was not subtracted
from the times &#8212; I report the time required for the pipeline to complete.
I report mean times per document, in milliseconds.</p>
<p><strong>Hardware</strong>: Intel i7-3770 (2012)</p>
<table border="1" class="docutils">
<caption>Per-document processing times.  Lower is better.</caption>
<colgroup>
<col width="20%" />
<col width="14%" />
<col width="12%" />
<col width="10%" />
<col width="14%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&nbsp;</td>
<td colspan="3">Absolute (ms per doc)</td>
<td colspan="3">Relative (to spaCy)</td>
</tr>
<tr class="row-even"><td>System</td>
<td>Tokenize</td>
<td>Tag</td>
<td>Parse</td>
<td>Tokenize</td>
<td>Tag</td>
<td>Parse</td>
</tr>
<tr class="row-odd"><td>spaCy</td>
<td>0.2ms</td>
<td>1ms</td>
<td>19ms</td>
<td>1x</td>
<td>1x</td>
<td>1x</td>
</tr>
<tr class="row-even"><td>CoreNLP</td>
<td>2ms</td>
<td>10ms</td>
<td>49ms</td>
<td>10x</td>
<td>10x</td>
<td>2.6x</td>
</tr>
<tr class="row-odd"><td>ZPar</td>
<td>1ms</td>
<td>8ms</td>
<td>850ms</td>
<td>5x</td>
<td>8x</td>
<td>44.7x</td>
</tr>
<tr class="row-even"><td>NLTK</td>
<td>4ms</td>
<td>443ms</td>
<td>n/a</td>
<td>20x</td>
<td>443x</td>
<td>n/a</td>
</tr>
</tbody>
</table>
<p>Efficiency is a major concern for NLP applications.  It is very common to hear
people say that they cannot afford more detailed processing, because their
datasets are too large.  This is a bad position to be in.  If you can&#8217;t apply
detailed processing, you generally have to cobble together various heuristics.
This normally takes a few iterations, and what you come up with will usually be
brittle and difficult to reason about.</p>
<p>spaCy&#8217;s parser is faster than most taggers, and its tokenizer is fast enough
for any workload.  And the tokenizer doesn&#8217;t just give you a list
of strings.  A spaCy token is a pointer to a Lexeme struct, from which you can
access a wide range of pre-computed features, including embedded word
representations.</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="quickstart.html" class="btn btn-neutral float-right" title="Quick Start">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Matthew Honnibal.
    </p>
  </div>

  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.85',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

  </body>
</html>