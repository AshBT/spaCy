

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>How spaCy Works &mdash; spaCy 0.85 documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Roboto+Slab:400,700|Inconsolata:400,700&subset=latin,cyrillic' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="spaCy 0.85 documentation" href="index.html"/>
        <link rel="next" title="License" href="license.html"/>
        <link rel="prev" title="API" href="api.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-58931649-1', 'gini.net');
      ga('send', 'pageview');
  </script>
  

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        
          <a href="index.html" class="fa fa-home"> spaCy</a>
        
        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">spaCy: Industrial-strength NLP</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#example-functionality">Example functionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#speed-comparison">Speed Comparison</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#accuracy-comparison">Accuracy Comparison</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#install">Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#usage">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#most-of-the-api-at-a-glance">(Most of the) API at a glance</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#features">Features</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">How spaCy Works</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tokenizer-and-lexicon">Tokenizer and Lexicon</a></li>
<li class="toctree-l2"><a class="reference internal" href="#part-of-speech-tagger">Part-of-speech Tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="#dependency-parser">Dependency Parser</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a><ul>
<li class="toctree-l2"><a class="reference internal" href="license.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="updates.html">Updates</a><ul>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-84">2015-05-12 v0.84</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-80">2015-04-13 v0.80</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#v0-70">2015-03-05 v0.70</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#spacy-v0-4-still-alpha-improving-quickly">2015-01-30 spaCy v0.4: Still alpha, improving quickly</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#bug-fixes">Bug Fixes</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#known-issues">Known Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#enhancements-train-and-evaluate-on-whole-paragraphs">Enhancements: Train and evaluate on whole paragraphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="updates.html#id1">2015-01-25</a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">spaCy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>How spaCy Works</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/howworks.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <div class="section" id="how-spacy-works">
<h1>How spaCy Works<a class="headerlink" href="#how-spacy-works" title="Permalink to this headline">¶</a></h1>
<p>The following are some hasty preliminary notes on how spaCy works.  The short
story is, there are no new killer algorithms.  The way that the tokenizer works
is novel and a bit neat, and the parser has a new feature set, but otherwise
the key algorithms are well known in the recent literature.</p>
<p>Some might also wonder how I get Python code to run so fast.  I don&#8217;t &#8212; spaCy
is written in <a class="reference external" href="http://cython.org/">Cython</a>, an optionally statically-typed language that compiles
to C or C++, which is then loaded as a C extension module.
This makes it <a class="reference external" href="https://honnibal.wordpress.com/2014/10/21/writing-c-in-cython/">easy to achieve the performance of native C code</a>, but allows the
use of Python language features, via the Python C API.  The Python unicode
library was particularly useful to me.  I think it would have been much more
difficult to write spaCy in another language.</p>
<div class="section" id="tokenizer-and-lexicon">
<h2>Tokenizer and Lexicon<a class="headerlink" href="#tokenizer-and-lexicon" title="Permalink to this headline">¶</a></h2>
<p>Tokenization is the task of splitting a string into meaningful pieces, called
tokens, which you can then compute with.  In practice, the task is usually to
match the tokenization performed in some treebank, or other corpus.  If we want
to apply a tagger, entity recogniser, parser etc, then we want our run-time
text to match the training conventions.  If we want to use a model that&#8217;s been
trained to expect &#8220;isn&#8217;t&#8221; to be split into two tokens, [&#8220;is&#8221;, &#8220;n&#8217;t&#8221;], then that&#8217;s
how we need to prepare our data.</p>
<p>In order to train spaCy&#8217;s models with the best data available, I therefore
tokenize English according to the Penn Treebank scheme.  It&#8217;s not perfect, but
it&#8217;s what everybody is using, and it&#8217;s good enough.</p>
<div class="section" id="what-we-don-t-do">
<h3>What we don&#8217;t do<a class="headerlink" href="#what-we-don-t-do" title="Permalink to this headline">¶</a></h3>
<p>The Penn Treebank was distributed with a script called tokenizer.sed, which
tokenizes ASCII newswire text roughly according to the Penn Treebank standard.
Almost all tokenizers are based on these regular expressions, with various
updates to account for unicode characters, and the fact that it&#8217;s no longer
1986 &#8212; today&#8217;s text has URLs, emails, emoji, etc.</p>
<p>Usually, the resulting regular expressions are applied in multiple passes, which
is quite inefficient.  Often no care is taken to preserve indices into the original
string.  If you lose these indices, it&#8217;ll be difficult to calculate mark-up based
on your annotations.</p>
</div>
<div class="section" id="tokenizer-algorithm">
<h3>Tokenizer Algorithm<a class="headerlink" href="#tokenizer-algorithm" title="Permalink to this headline">¶</a></h3>
<p>spaCy&#8217;s tokenizer assumes that no tokens will cross whitespace &#8212; there will
be no multi-word tokens.  If we want these, we can post-process the
token-stream later, merging as necessary.  This assumption allows us to deal
only with small chunks of text.  We can cache the processing of these, and
simplify our expressions somewhat.</p>
<p>Here is what the outer-loop would look like in Python. (You can see the
production implementation, in Cython, here.)</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="n">cache</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">substring</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">substring</span> <span class="ow">in</span> <span class="n">cache</span><span class="p">:</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="n">substring</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">subtokens</span> <span class="o">=</span> <span class="n">_tokenize_substring</span><span class="p">(</span><span class="n">substring</span><span class="p">)</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">subtokens</span><span class="p">)</span>
            <span class="n">cache</span><span class="p">[</span><span class="n">substring</span><span class="p">]</span> <span class="o">=</span> <span class="n">subtokens</span>
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>
</div>
<p>The actual work is performed in _tokenize_substring.  For this, I divide the
tokenization rules into three pieces:</p>
<ol class="arabic simple">
<li>A prefixes expression, which matches from the start of the string;</li>
<li>A suffixes expression, which matches from the end of the string;</li>
<li>A special-cases table, which matches the whole string.</li>
</ol>
<p>The algorithm then proceeds roughly like this (consider this like pseudo-code;
this was written quickly and has not been executed):</p>
<div class="code python highlight-python"><div class="highlight"><pre># Tokens which can be attached at the beginning or end of another
prefix_re = _make_re([&quot;,&quot;, &#39;&quot;&#39;, &#39;(&#39;, ...])
suffix_re = _make_re(s[&quot;,&quot;, &quot;&#39;&quot;, &quot;:&quot;, &quot;&#39;s&quot;, ...])

# Contractions etc are simply enumerated, since they&#39;re a finite set.  We
# can also specify anything we like here, which is nice --- different data
# has different quirks, so we want to be able to add ad hoc exceptions.
special_cases = {
    &quot;can&#39;t&quot;: (&quot;ca&quot;, &quot;n&#39;t&quot;),
    &quot;won&#39;t&quot;: (&quot;wo&quot;, &quot;n&#39;t&quot;),
    &quot;he&#39;d&#39;ve&quot;: (&quot;he&quot;, &quot;&#39;d&quot;, &quot;&#39;ve&quot;),
    ...
    &quot;:)&quot;: (&quot;:)&quot;,) # We can add any arbitrary thing to this list.
}

def _tokenize_substring(substring):
    prefixes = []
    suffixes = []
    while substring not in special_cases:
        prefix, substring = _apply_re(substring, prefix_re)
        if prefix:
            prefixes.append(prefix)
        else:
            suffix, substring = _apply_re(substring, suffix_re)
            if suffix:
                suffixes.append(suffix)
            else:
                break
</pre></div>
</div>
<p>This procedure splits off tokens from the start and end of the string, at each
point checking whether the remaining string is in our special-cases table.  If
it is, we stop splitting, and return the tokenization at that point.</p>
<p>The advantage of this design is that the prefixes, suffixes and special-cases
can be declared separately, in easy-to-understand files.  If a new entry is
added to the special-cases, you can be sure that it won&#8217;t have some unforeseen
consequence to a complicated regular-expression grammar.</p>
</div>
<div class="section" id="coupling-the-tokenizer-and-lexicon">
<h3>Coupling the Tokenizer and Lexicon<a class="headerlink" href="#coupling-the-tokenizer-and-lexicon" title="Permalink to this headline">¶</a></h3>
<p>As mentioned above, the tokenizer is designed to support easy caching.  If all
we were caching were the matched substrings, this would not be so advantageous.
Instead, what we do is create a struct which houses all of our lexical
features, and cache <em>that</em>.  The tokens are then simply pointers to these rich
lexical types.</p>
<p>In a sample of text, vocabulary size grows exponentially slower than word
count.  So any computations we can perform over the vocabulary and apply to the
word count are efficient.</p>
</div>
</div>
<div class="section" id="part-of-speech-tagger">
<h2>Part-of-speech Tagger<a class="headerlink" href="#part-of-speech-tagger" title="Permalink to this headline">¶</a></h2>
<p>In 2013, I wrote a blog post describing <a class="reference external" href="https://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/.">how to write a good part of speech
tagger</a>.
My recommendation then was to use greedy decoding with the averaged perceptron.
I think this is still the best approach, so it&#8217;s what I implemented in spaCy.</p>
<p>The tutorial also recommends the use of Brown cluster features, and case
normalization features, as these make the model more robust and domain
independent.  spaCy&#8217;s tagger makes heavy use of these features.</p>
</div>
<div class="section" id="dependency-parser">
<h2>Dependency Parser<a class="headerlink" href="#dependency-parser" title="Permalink to this headline">¶</a></h2>
<p>The parser uses the algorithm described in my <a class="reference external" href="https://honnibal.wordpress.com/2013/12/18/a-simple-fast-algorithm-for-natural-language-dependency-parsing/">2014 blog post</a>.
This algorithm, shift-reduce dependency parsing, is becoming widely adopted due
to its compelling speed/accuracy trade-off.</p>
<p>Some quick details about spaCy&#8217;s take on this, for those who happen to know
these models well.  I&#8217;ll write up a better description shortly.</p>
<ol class="arabic simple">
<li>I use greedy decoding, not beam search;</li>
<li>I use the arc-eager transition system;</li>
<li>I use the Goldberg and Nivre (2012) dynamic oracle.</li>
<li>I use the non-monotonic update from my CoNLL 2013 paper (Honnibal, Goldberg
and Johnson 2013).</li>
</ol>
<p>So far, this is exactly the configuration from the CoNLL 2013 paper, which
scored 91.0. So how have I gotten it to 92.4?  The following tweaks:</p>
<ol class="arabic simple">
<li>I use Brown cluster features &#8212; these help a lot;</li>
<li>I redesigned the feature set. I&#8217;ve long known that the Zhang and Nivre
(2011) feature set was suboptimal, but a few features don&#8217;t make a very
compelling publication.  Still, they&#8217;re important.</li>
<li>When I do the dynamic oracle training, I also make
the upate cost-sensitive: if the oracle determines that the move the parser
took has a cost of N, then the weights for the gold class are incremented by
+N, and the weights for the predicted class are incremented by -N.  This
only made a small (0.1-0.2%) difference.</li>
</ol>
<div class="section" id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h3>
<p>I don&#8217;t do anything algorithmically novel to improve the efficiency of the
parser.  However, I was very careful in the implementation.</p>
<p>A greedy shift-reduce parser with a linear model boils down to the following
loop:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">feature_funcs</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">init_state</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">templ</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="k">for</span> <span class="n">templ</span> <span class="ow">in</span> <span class="n">feature_funcs</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="n">feat</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
                <span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">weight</span>
        <span class="n">class_</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">transition</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">class_</span><span class="p">)</span>
</pre></div>
</div>
<p>The parser makes 2N transitions for a sentence of length N.  In order to select
the transition, it extracts a vector of K features from the state.  Each feature
is used as a key into a hash table managed by the model.  The features map to
a vector of weights, of length C.  We then dot product the feature weights to the
scores vector we are building for that instance.</p>
<p>The inner-most loop here is not so bad: we only have a few dozen classes, so
it&#8217;s just a short dot product.  Both of the vectors are in the cache, so this
is a snack to a modern CPU.</p>
<p>The bottle-neck in this algorithm is the 2NK look-ups into the hash-table that
we must make, as these almost always have to hit main memory.  The feature-set
is enormously large, because all of our features are one-hot boolean
indicators.  Some of the features will be common, so they&#8217;ll lurk around in the
CPU&#8217;s cache hierarchy.  But a lot of them won&#8217;t be, and accessing main memory
takes a lot of cycles.</p>
<p>I used to use the Google dense_hash_map implementation.  This seemed a solid
choice: it came from a big brand, it was in C++, and it seemed very
complicated.  Later, I read <a class="reference external" href="http://preshing.com/20130107/this-hash-table-is-faster-than-a-judy-array/.">Jeff Preshing&#8217;s excellent post</a> on open-addressing
with linear probing.
This really spoke to me.  I had assumed that a fast hash table implementation
would necessarily be very complicated, but no &#8212; this is another situation
where the simple strategy wins.</p>
<p>I&#8217;ve packaged my Cython implementation separately from spaCy, in the package
<a class="reference external" href="https://github.com/syllog1sm/preshed">preshed</a> &#8212; for &#8220;pre-hashed&#8221;, but also as a nod to Preshing.  I&#8217;ve also taken
great care over the feature extraction and perceptron code, which I&#8217;m distributing
in a package named <a class="reference external" href="https://github.com/honnibal/thinc">thinc</a> (since it&#8217;s for learning very sparse models with
Cython).</p>
<p>By the way: from comparing notes with a few people, it seems common to
implement linear models in a way that&#8217;s suboptimal for multi-class
classification.  The mistake is to store in the hash-table one weight per
(feature, class) pair, rather than mapping the feature to a vector of weights,
for all of the classes.  This is bad because it means you need to hit the table
C times, one per class, as you always need to evaluate a feature against all of
the classes.  In the case of the parser, this means the hash table is accessed
2NKC times, instead of the 2NK times if you have a weights vector.  You should
also be careful to store the weights contiguously in memory &#8212; you don&#8217;t want
a linked list here.  I use a block-sparse format, because my problems tend to
have a few dozen classes.</p>
<p>I guess if I had to summarize my experience, I&#8217;d say that the efficiency of
these models is really all about the data structures.  We want to stay small,
and stay contiguous.  Minimize redundancy and minimize pointer chasing.
That&#8217;s why Cython is so well suited to this: we get to lay out our data
structures, and manage the memory ourselves, with full C-level control.</p>
</div>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="license.html" class="btn btn-neutral float-right" title="License">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="api.html" class="btn btn-neutral" title="API"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Matthew Honnibal.
    </p>
  </div>

  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.85',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

  </body>
</html>